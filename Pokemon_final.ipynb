{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZiACedWQ2QVJW1V1QPN5c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HBLazerr/pokemon-data-analysis/blob/main/Pokemon_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pokemon Dataset**"
      ],
      "metadata": {
        "id": "Z0Jx5qWy9Ptl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a Pokemon dataset which contains details about all 802 Pokemon. It includes stats like speed, HP, legendary status, attack, generations, abilities, special attacks, type, classification, etc. The data is from a Kaggle dataset I found from an article under good datasets for analysis. This dataset is a good choice because it allows for EDA and everything mentioned in the requirements. More specifically, I was thinking I can test to see which stats are the strongest in affecting whether or not a Pokemon is legendary and seeing what I can find."
      ],
      "metadata": {
        "id": "yD85wew79WeH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Dataset**"
      ],
      "metadata": {
        "id": "EP9eDN2YTPwG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jtMfNPIg90CW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "5b8b8014-77fd-431e-aa16-6960a8d5d739"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'pokemon.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3a03af4e3d2c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pokemon.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Quick check to make sure dataset loaded correctly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'pokemon.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('pokemon.csv')\n",
        "\n",
        "# Quick check to make sure dataset loaded correctly\n",
        "print(data.head()) # first 5 rows"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Cleaning**"
      ],
      "metadata": {
        "id": "40FPwHE7LrjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing values for type2 column w 'None'\n",
        "# some pokemons do not have secondary types so thats why we have empty values to replace\n",
        "data.fillna({'type2': 'None'}, inplace=True)\n",
        "\n",
        "# Check again if any values are missing in dataset\n",
        "print(\"\\nMissing Values After Cleaning:\\n\")\n",
        "print(data.isnull().sum())"
      ],
      "metadata": {
        "id": "_-OVecyhL5t3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exploratory Data Analysis (EDA)**"
      ],
      "metadata": {
        "id": "xKR-3QcFTaTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary statistics for numerical columns (mean, max, min, std, etc.)\n",
        "print(\"\\nSummary Statistics:\\n\")\n",
        "print(data.describe())\n",
        "\n",
        "# Get general dataset info (rows, columns, total elements, data types)\n",
        "print(f\"{data.shape[0]} Rows, {data.shape[1]} Columns\")\n",
        "print(f\"Total elements: {data.size}\")\n",
        "print(\"\\nAll column data types:\\n\")\n",
        "print(data.dtypes)\n",
        "\n",
        "# Check for/if any empty values\n",
        "print(\"\\nMissing Values:\\n\")\n",
        "print(data.isnull().sum())\n",
        "\n",
        "# Get counts of pokemon in every generation\n",
        "print(\"\\nNumber of Pokemon in each Generation:\")\n",
        "print(data['generation'].value_counts().sort_index())  # sort by generation (1-5)\n",
        "# counts of legendary pokemon in every generation\n",
        "print(\"\\nLegendary Pokemon in each generation:\")\n",
        "print(data[data['is_legendary'] == 1]['generation'].value_counts().sort_index())\n",
        "# Legendary and Non legendary pokemon\n",
        "print(\"\\nLegendary vs Non Legendary Pokemon (0: Non Legendary, 1: Legendary):\")\n",
        "print(data['is_legendary'].value_counts())"
      ],
      "metadata": {
        "id": "m3f4GOeqTcjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PLOTS - Exploratory Data Analysis (EDA)**"
      ],
      "metadata": {
        "id": "Tg2Vn3WkGkCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Combine 'type1' and 'type2' into one col\n",
        "# AKA the Pokemon's primary (1) and secondary (2) types\n",
        "plt.figure(figsize=(8, 6)) # 8x6\n",
        "types_counts = pd.concat([data['type1'], data['type2']]).value_counts()  # count num of combined types\n",
        "sns.barplot(x=types_counts.index[:10], y=types_counts.values[:10])  # get just the highest 10\n",
        "# titles & labels\n",
        "plt.title('Top 10 Pokemon types')\n",
        "plt.xlabel('Type', fontweight='bold')\n",
        "plt.ylabel('Count', fontweight='bold')\n",
        "plt.show()\n",
        "\n",
        "# Heatmap of the pokemon datasets number columns vs each other\n",
        "plt.figure(figsize=(10, 8)) # 10x8\n",
        "# .corr() - calcs each columns correlation against all the other columns\n",
        "corr_matrix = data[['hp', 'attack', 'defense', 'sp_attack', 'sp_defense', 'speed', 'base_total']].corr()\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.show()\n",
        "\n",
        "# Legendary vs Non Legendary Plot for each gen\n",
        "# this is to see which gens have the most legendary Pokemon seeing if generation might have anything to do with it being legendary\n",
        "plt.figure(figsize=(8, 5)) # 8x5\n",
        "sns.countplot(data=data, x='generation', hue='is_legendary')  # makes it diff colors\n",
        "# titles & labels\n",
        "plt.title('Legendary vs Non Legendary by Generation')\n",
        "plt.xlabel('Generation', fontweight='bold')\n",
        "plt.ylabel('Count', fontweight='bold')\n",
        "plt.legend(title=\"legendary\", labels=[\"No\", \"Yes\"])  # replace 0 & 1 w No & Yes\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uwz-SqYgGqzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Machine Learning**"
      ],
      "metadata": {
        "id": "mDyTgytKNBnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Creates feature and target variable\n",
        "target = 'is_legendary'  # this is the column to predict\n",
        "features = ['hp', 'attack', 'defense', 'sp_attack', 'sp_defense', 'speed', 'base_total']\n",
        "\n",
        "X = data[features]  # Feature (predictor)\n",
        "y = data[target]  # Target\n",
        "\n",
        "# split dataset into 80% being training and 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # for reproducing\n",
        "\n",
        "# Initializes model and fits to training data from above\n",
        "model = DecisionTreeClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on test set\n",
        "y_pred = model.predict(X_test)  # predict target values"
      ],
      "metadata": {
        "id": "WKIXWAoeNIqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Performance**"
      ],
      "metadata": {
        "id": "IfHDQgeCOiPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Eval model performance w the classificaton report for each class\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred))  # precision, recall, F1-score\n",
        "# Show confusion matrix to see how well it predicted\n",
        "print(\"\\nConfusion Matrix:\\n\")\n",
        "print(confusion_matrix(y_test, y_pred))  # actual vs predicted\n",
        "\n",
        "print(\"\\n Accuracy:\\n\")\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "\n",
        "# this is to visualize the feature importance to see which stats affect the predictions the most\n",
        "feature_importance = model.feature_importances_  # gets importance scores\n",
        "plt.figure(figsize=(10, 6)) # 10x6\n",
        "sns.barplot(x=feature_importance, y=X.columns)\n",
        "# labels\n",
        "plt.xlabel('Importance', fontweight='bold')\n",
        "plt.ylabel('Features', fontweight='bold')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NWhzzXzbOi4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insights**"
      ],
      "metadata": {
        "id": "SvElckW9G2ly"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the overall analysis of the pokemon dataset, there were a few interesting things that I noticed. The data had 801 rows and 41 columns which were pretty clean as it only had few missing values scattered around mainly in type2, height_m, and weight_kg. To clean this up, I decided to just fill type2 with the value 'None' since not all pokemon have a secondary type that way it wouldn't mess up when I throw off my result like when I combined each Pokemon's primary and secondary types to find the most common types which ended up being water and normal. From the EDA part of it, I saw that generation five had the most Pokémon overall, but Generations four along with five had the highest number of Legendary Pokémon which I thought was interesting. In my heatmap, you can see that base_total was the highest correlated with sp_attack, attack, and sp_defense stats so my guess is these make up most of the Pokemon's total. The model itself got a score of 95.6% with precision, recall, and the F1 score being higher for non legendary compared to the legendaries. The feature importance plot I ended up adding last minute confirmed that the base total was way overly correlated with stats compared to the rest but this was because base_total is of all of them in one. The runner ups were hp, sp_attack, and attack. Overall though the model performed well in findng patterns within the dataset and proving that specials and attacks strongly added to whether or not the Pokemon is legendary."
      ],
      "metadata": {
        "id": "-XA38SZcG302"
      }
    }
  ]
}